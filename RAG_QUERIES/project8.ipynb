{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multi Representation Indexing\n",
    "\"\"\"\n",
    " Input: Full Doc, Table, or Image\n",
    "You start with some content ‚Äî a document, table, or image.\n",
    "\n",
    "This could be, for example, a PDF report, an Excel table, or an image of a chart.\n",
    "\n",
    "2. GPT-4V Processing\n",
    "GPT-4V (Vision model) processes this raw content.\n",
    "\n",
    "It generates a summary ‚Äî a textual abstraction of the content's key ideas.\n",
    "\n",
    "This summary is easier to embed and index than the raw content.\n",
    "\n",
    "3. Multi-representation Indexing Block\n",
    "This has two parts:\n",
    "\n",
    "a. Vectorstore\n",
    "The summary is passed through an embedding model (converts it into a numerical vector).\n",
    "\n",
    "These vectors are stored in the Vectorstore.\n",
    "\n",
    "When a question comes in, it is also embedded and compared to the stored vectors to find the most similar content.\n",
    "\n",
    "b. Docstore\n",
    "This stores the original full content (document, table, or image).\n",
    "\n",
    "When a match is found in the vectorstore, we retrieve the actual content from the docstore ‚Äî not just the summary.\n",
    "\n",
    "‚ùì Question Flow:\n",
    "A user asks a question.\n",
    "\n",
    "The system embeds the question and searches the Vectorstore for similar summaries.\n",
    "\n",
    "Once it finds the best match, it uses that to retrieve the corresponding original full document/table/image from the Docstore.\n",
    "\n",
    "The result returned is the full, relevant content, not just a snippet.\n",
    "\n",
    "üì¶ Output:\n",
    "‚û°Ô∏è Relevant full doc, table, or image based on the user's query.\n",
    "\n",
    "‚úÖ Key Advantages:\n",
    "Fast similarity search using vector embeddings.\n",
    "\n",
    "Rich and precise retrieval since it still keeps the full source content.\n",
    "\n",
    "Uses multi-modal understanding (via GPT-4V) to work even with images or tables, not just text.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#for example we are taking two contents from the blog and load to convert them into docs\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())\n",
    "\n",
    "#we are creating a cchain that genertes summaries for the list of docs that we giev\n",
    "import uuid\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo\",max_retries=0)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "# The vectorstore to use to index the child chunks initialisg a vectorspace to store the summary embeddings\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=OpenAIEmbeddings())\n",
    "# The storage layer for the parent documents we store the actual emebedding sof the parent document in the memoryBYstore\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "# The retriever inbuilt which will find the summary and get the releveant original doc \n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]#create a list of unique id's \n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})#here from the normal docs wth page content we are creating anew object metadata about the content with content and id as attributes\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "# Add the created spaces to the onejct created previously\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working\n",
    "query=\"Memory In agents\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will discuss about the next Method RAPTOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In ColBERT, documents and queries are tokenized and embedded using a transformer like BERT. Each query token is compared with all document tokens,\n",
    " and for each query token, the document token with the highest similarity is selected. These similarities are then aggregated to rank documents,\n",
    "enabling fine-grained and efficient retrieval.\n",
    "So the output will be not the entire docs but the part of docs corresponding to the max_similarity tokens \n",
    "\"\"\"\n",
    "from ragatouille import RAGPretrainedModel\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "import requests #this is like a router to fetch or post things to an api here from the wiki pedia we are gonna load anf give extract as text\n",
    "def get_wikipedia_page(title: str):\n",
    "    \"\"\"\n",
    "    Retrieve the full text content of a Wikipedia page.\n",
    "\n",
    "    :param title: str - Title of the Wikipedia page.\n",
    "    :return: str - Full text content of the page as raw string.\n",
    "    \"\"\"\n",
    "    # Wikipedia API endpoint\n",
    "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"titles\": title,\n",
    "        \"prop\": \"extracts\",\n",
    "        \"explaintext\": True,\n",
    "    }\n",
    "\n",
    "    # Custom User-Agent header to comply with Wikipedia's best practices\n",
    "    headers = {\"User-Agent\": \"RAGatouille_tutorial/0.0.1 (ben@clavie.eu)\"}\n",
    "\n",
    "    response = requests.get(URL, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    # Extracting page content\n",
    "    page = next(iter(data[\"query\"][\"pages\"].values()))\n",
    "    return page[\"extract\"] if \"extract\" in page else None\n",
    "\n",
    "full_document = get_wikipedia_page(\"Hayao_Miyazaki\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
