{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#o this just a basic part of using a indexing,retrievel and generation of an RAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = <your-api-key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the first part is indexing (representation of strings into embeddings for effective an easy search)\n",
    "# this is just an exmaple for one query and one document but this is for study mainly we follow mant processes for embedding \n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)\n",
    "\n",
    "#main chain of processes\n",
    "#1)\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(#this bs4 is a lib which is used for scraping only the necessary content from the web to feed it to the llm now ffrom the blog title we onlt get the htl divisons mentioned wrt to the class here and we can also iterate /parse through the html loaded using beautifulSoup\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()#this loads the html docs from the link then convert it into docs format\n",
    "\n",
    "#2)Spliiting into chunks while ensuring semantics\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "#3)Storing\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "#retriever = vectorstore.as_retriever()\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})#so this is like  k nearest neighbours where you collect th k most semantically nearest vector chunk fromthe Chroma vectorbase\n",
    "#docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")#example call\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation part where the llm walks in\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)#so you can use this to give a proper prompt instead of justa  mere string\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "#Now we are gonna create a chain here \n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}#the important point to be noted here is that the doccs from the response is given as a parameter internally between the pipes of the chain so it is very simple\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()#for parsing the string output\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")# when we call this invoke function with a string retreiver(\"str\") will be called and the response is stored in content then question is just assigned to the string\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
